<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>RoundTable</title>
    <link>https://rroundtable.github.io/</link>
    <description>Recent content on RoundTable</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>�� This post is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License��please give source if you wish to quote or reproduce.</copyright>
    <lastBuildDate>Thu, 11 Jul 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://rroundtable.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>How Does Batch Normalization Help Optimization? 정리글</title>
      <link>https://rroundtable.github.io/post/2019-07-11-how-does-batch-normalization-help-optimization/</link>
      <pubDate>Thu, 11 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://rroundtable.github.io/post/2019-07-11-how-does-batch-normalization-help-optimization/</guid>
      <description>Main Contribution Batch normalization에 대하여에서 BN이 결국 internal covariate shift현상을 해결하여, 모델의 수렴속도를 높인다고 주장하였다. 하지만, 해당 논문에서는 internal covariate shift현상을 감소하여 그러는 것이 아니며, BN이 실제로 감소시키지 않는다고 주장한다.
이 논문에서는 BN이 optimization problem을 smoother하게 만들어서 성공적이라고 주장한다. 이로 인해서 gradient는 predictive해지고 더 큰 learning rate를 사용할 수 있다.
 optimization problem이 smoother 해진다는 것은&amp;hellip;
https://ifm.mathematik.uni-wuerzburg.de/~schmidt/publications.php
 Batch normalization and internal covariate shift train, test 그래프에서는 batch normalization의 역할을 잘 보여주고 있다.</description>
    </item>
    
    <item>
      <title>What Uncertainties Do We Need in Bayesian Deep 정리글 Learning for Computer Vision? </title>
      <link>https://rroundtable.github.io/post/2019-06-29-what-uncertainties-do-we-need-in-bayesian-deep/</link>
      <pubDate>Sat, 29 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://rroundtable.github.io/post/2019-06-29-what-uncertainties-do-we-need-in-bayesian-deep/</guid>
      <description>이 논문에서는 epistemic uncertainty와 aleatoric uncertainty를 하나의 모델에서 측정하는 것을 제안하고 있습니다. (이전의 연구에서는 위의 uncertainty를 따로 분리하여 측정했다고 합니다.)
  aleatoric uncertainty와 epistemic uncertainty의 차이를 보여주고 있다. 주된 차이점은 aleatoric은 물체사이의 boundary에 주로 나타나는 것을 확인할 수 있다. 맨 밑의 라인은 실패한 케이스를 보여준다. 여기서는 epistemic uncertainty가 높아진 것을 확인할 수 있다.  regression task에서 각각의 uncertainty에 대해서 알아보도록 하겠습니다.
Epistemic uncertainty  Fig.1 - Gaussian Process</description>
    </item>
    
    <item>
      <title>about me</title>
      <link>https://rroundtable.github.io/post/aboutme/</link>
      <pubDate>Fri, 28 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://rroundtable.github.io/post/aboutme/</guid>
      <description>Ryu Won-Tak PROJECTS Mixup implementation Data augmentation via convex combination.
Hedged instance embedding implementation Modeled uncertainty by hedging the location of each input in embedding space.
[low uncertainty in clean image] [high uncertainty in occluded image]
Error encoding network implementation Visualized that the distance of different latent variables measures the similarity of images. **Mixture density network implementation ** Visualized explainable variance and unexplainable variance. AWARDS AND HONORS  Ocean litter detector</description>
    </item>
    
    <item>
      <title>Neural Networks, Manifolds, and Topology 번역글</title>
      <link>https://rroundtable.github.io/post/neural-networks-manifolds-and-topology-/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://rroundtable.github.io/post/neural-networks-manifolds-and-topology-/</guid>
      <description>Neural Networks, Manifolds, and Topology Neural network가 실제로 어떻게 작동하는지 파악하는 것은 실제로 어려운 일입니다. 그래서 black-box 모델이라고 종종 말하곤 합니다.
이런 문제를 해결하고자 이번 글에서는 topology(math with shapes)와 neural network간의 관계를 살펴보고자 합니다. topology를 처음 접하신다면 해당 Who cares about topology?를 보시는 것을 추천드립니다.
topology를 언급하는 이유는 representation space상에서 공간의 변화가 일어나도 같은 성질을 가지고 있음을 증명하기 위해서입니다.
 Fig.1 - same topology  
도넛과 물컵은 같은 topology를 가진다.</description>
    </item>
    
    <item>
      <title>Batch Normalization에 대하여</title>
      <link>https://rroundtable.github.io/post/2019-07-08-batch-normalization%EC%97%90-%EB%8C%80%ED%95%98%EC%97%AC/</link>
      <pubDate>Mon, 08 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://rroundtable.github.io/post/2019-07-08-batch-normalization%EC%97%90-%EB%8C%80%ED%95%98%EC%97%AC/</guid>
      <description>Problem Define 학습하는 과정에서 이전 layer의 parameter가 변하면서, 각 layer의 input들의 distribution이 training과정마다 변하게 된다. 이런 문제는 학습이 불안정하게 하며, 낮은 learning rate를 사용해야 학습이 진행된다. 결론적으로는 saturating non-linearity의 모델을 학습하기 어려워진다. 이런 현상을 internal covariate shift 라고 부른다.
 saturating non-linearity: 어떤 입력이 무한대로 갈 때 함수값이 어떤 범위내에서만 움직이는 것
ex) sigmoid
not-saturating non-linearity: 어떤 입력이 무한대로 갈 때 함수값도 무한대로 가는 것을 의미
ex) Relu
 sigmoid activation에 대해서 생각해보면, 위의 문제가 왜 심각한지 알 수 있다.</description>
    </item>
    
    <item>
      <title>MLE 와 MAP에 대하여</title>
      <link>https://rroundtable.github.io/post/mle-maximum-likelihood-estimation/</link>
      <pubDate>Sun, 07 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://rroundtable.github.io/post/mle-maximum-likelihood-estimation/</guid>
      <description>MLE: Maximum Likelihood Estimation *liklihood*란, 이미 주어진 표본적 증거로 비추어보았을 때, 모집단에 관해 어떠한 통계적 추정이 그럴듯한 정도를 의미합니다. 수식으로 나타내면, $P(D|W)$ 처럼 나타낼 수 있다. (D = observation, W = parameters) .
동전던지기 예시를 생각해보면, 쉽게 이해할 수 있다. 일반적으로 동전 던지기를 해서 앞면이 나올 확률은 0.5라고 생각합니다. 하지만, 이는 우리가 가정한 값이지 실제의 값은 아닙니다. 이런 이유로 몇 번의 수행결과로 동전의 앞면이 나올 확률 $P(H)$를 정의하고자 합니다.</description>
    </item>
    
    <item>
      <title>RKHS에 대하여</title>
      <link>https://rroundtable.github.io/post/rkhs/</link>
      <pubDate>Fri, 05 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://rroundtable.github.io/post/rkhs/</guid>
      <description>Hibert space  A Hilbert space($\mathbf{H}$) is a complete inner product linear space.
  Linear space  선형공간이라는 것은 임의의 두 원소 $x$,$ y$의 선형결합 또한 원소로 가지고 있어야 하는 조건이다. $$ \alpha x + \beta y \in \mathbf{H}(\alpha, \beta: scalar) $$ 2. Inner product
힐버트 공간에서는 내적으로 metric을 정의한다.
내적 $$는 $H \times H$에서 $\mathbb{R}$로의 bilinear positive definite kernel이다.
 bilinear: $x, y$ 둘 중 하나를 고정하면 다른 하나에 대한 linear 함수   positive definite    Complete  힐버트 공간 $\mathcal{H}$내의 어떠한 Cauchy sequence도 $\mathcal{H}$내의 원소로 수렴한다.</description>
    </item>
    
  </channel>
</rss>