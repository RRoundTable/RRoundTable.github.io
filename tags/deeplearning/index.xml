<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>deeplearning on Deep learning Blog</title>
    <link>https://rroundtable.github.io/tags/deeplearning/</link>
    <description>Recent content in deeplearning on Deep learning Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>�� This post is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License��please give source if you wish to quote or reproduce.</copyright>
    <lastBuildDate>Sat, 29 Jun 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://rroundtable.github.io/tags/deeplearning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>What Uncertainties Do We Need in Bayesian Deep 정리글 Learning for Computer Vision? </title>
      <link>https://rroundtable.github.io/post/2019-06-29-what-uncertainties-do-we-need-in-bayesian-deep/</link>
      <pubDate>Sat, 29 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://rroundtable.github.io/post/2019-06-29-what-uncertainties-do-we-need-in-bayesian-deep/</guid>
      <description>이 논문에서는 epistemic uncertainty와 aleatoric uncertainty를 하나의 모델에서 측정하는 것을 제안하고 있습니다. (이전의 연구에서는 위의 uncertainty를 따로 분리하여 측정했다고 합니다.)
  aleatoric uncertainty와 epistemic uncertainty의 차이를 보여주고 있다. 주된 차이점은 aleatoric은 물체사이의 boundary에 주로 나타나는 것을 확인할 수 있다. 맨 밑의 라인은 실패한 케이스를 보여준다. 여기서는 epistemic uncertainty가 높아진 것을 확인할 수 있다.  regression task에서 각각의 uncertainty에 대해서 알아보도록 하겠습니다.
Epistemic uncertainty  Fig.1 - Gaussian Process</description>
    </item>
    
    <item>
      <title>Neural Networks, Manifolds, and Topology 번역글</title>
      <link>https://rroundtable.github.io/post/neural-networks-manifolds-and-topology-/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://rroundtable.github.io/post/neural-networks-manifolds-and-topology-/</guid>
      <description>Neural Networks, Manifolds, and Topology Neural network가 실제로 어떻게 작동하는지 파악하는 것은 실제로 어려운 일입니다. 그래서 black-box 모델이라고 종종 말하곤 합니다.
이런 문제를 해결하고자 이번 글에서는 topology(math with shapes)와 neural network간의 관계를 살펴보고자 합니다. topology를 처음 접하신다면 해당 Who cares about topology?를 보시는 것을 추천드립니다.
topology를 언급하는 이유는 representation space상에서 공간의 변화가 일어나도 같은 성질을 가지고 있음을 증명하기 위해서입니다.
도넛과 물컵은 같은 topology를 가진다. 왜냐하면 구멍이 하나이기 때문에.
유사해 보이지만 서로 다른 topology를 가진다.</description>
    </item>
    
  </channel>
</rss>