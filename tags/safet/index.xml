<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>safet on RoundTable</title>
    <link>https://rroundtable.github.io/tags/safet/</link>
    <description>Recent content in safet on RoundTable</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>�� This post is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License��please give source if you wish to quote or reproduce.</copyright>
    <lastBuildDate>Sun, 21 Jul 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://rroundtable.github.io/tags/safet/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Mixup 정리글</title>
      <link>https://rroundtable.github.io/post/mixup-467e0a5d-4d28-4e05-a587-9007b9d1b97f/</link>
      <pubDate>Sun, 21 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://rroundtable.github.io/post/mixup-467e0a5d-4d28-4e05-a587-9007b9d1b97f/</guid>
      <description>Mixup: Beyond Empirical Risk Minimization mixup은 deep learning model의 memorization문제나 adversarial examples에 민감한 이슈를 해결하기 위해 나온 Data Augmentation기법입니다.
 memorization:  모델이 학습을 진행할 때, 정답만을 기억하고 내리는 행동. 즉, 데이터 분포를 학습하는 것이 아니라 해당 데이터가 어떤 라벨에 해당하는지 기억하게 되는 것. 결론적으로는 test distribution에 대한 generalization을 하지 못한다.
 sensitivity to adversarial examples: adversarial attack에 취약하다.  mixup은 network를 data pair간의 convex combination을 이용하여 학습시킵니다. 이는 결과적으로 모델이 training sample간에 simple linear behavior를 하지 않도록 하는 효과가 있습니다.</description>
    </item>
    
  </channel>
</rss>