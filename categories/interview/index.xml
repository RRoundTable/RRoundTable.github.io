<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>interview on RoundTable</title>
    <link>https://rroundtable.github.io/categories/interview/</link>
    <description>Recent content in interview on RoundTable</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>�� This post is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License��please give source if you wish to quote or reproduce.</copyright>
    <lastBuildDate>Mon, 08 Jul 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://rroundtable.github.io/categories/interview/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Batch Normalization에 대하여</title>
      <link>https://rroundtable.github.io/post/2019-07-08-batch-normalization%EC%97%90-%EB%8C%80%ED%95%98%EC%97%AC/</link>
      <pubDate>Mon, 08 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://rroundtable.github.io/post/2019-07-08-batch-normalization%EC%97%90-%EB%8C%80%ED%95%98%EC%97%AC/</guid>
      <description>Problem Define 학습하는 과정에서 이전 layer의 parameter가 변하면서, 각 layer의 input들의 distribution이 training과정마다 변하게 된다. 이런 문제는 학습이 불안정하게 하며, 낮은 learning rate를 사용해야 학습이 진행된다. 결론적으로는 saturating non-linearity의 모델을 학습하기 어려워진다. 이런 현상을 internal covariate shift 라고 부른다.
 saturating non-linearity: 어떤 입력이 무한대로 갈 때 함수값이 어떤 범위내에서만 움직이는 것
ex) sigmoid
not-saturating non-linearity: 어떤 입력이 무한대로 갈 때 함수값도 무한대로 가는 것을 의미
ex) Relu
 sigmoid activation에 대해서 생각해보면, 위의 문제가 왜 심각한지 알 수 있다.</description>
    </item>
    
    <item>
      <title>MLE 와 MAP에 대하여</title>
      <link>https://rroundtable.github.io/post/mle-maximum-likelihood-estimation/</link>
      <pubDate>Sun, 07 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://rroundtable.github.io/post/mle-maximum-likelihood-estimation/</guid>
      <description>MLE: Maximum Likelihood Estimation *liklihood*란, 이미 주어진 표본적 증거로 비추어보았을 때, 모집단에 관해 어떠한 통계적 추정이 그럴듯한 정도를 의미합니다. 수식으로 나타내면, $P(D|W)$ 처럼 나타낼 수 있다. (D = observation, W = parameters) .
동전던지기 예시를 생각해보면, 쉽게 이해할 수 있다. 일반적으로 동전 던지기를 해서 앞면이 나올 확률은 0.5라고 생각합니다. 하지만, 이는 우리가 가정한 값이지 실제의 값은 아닙니다. 이런 이유로 몇 번의 수행결과로 동전의 앞면이 나올 확률 $P(H)$를 정의하고자 합니다.</description>
    </item>
    
    <item>
      <title>RKHS에 대하여</title>
      <link>https://rroundtable.github.io/post/rkhs/</link>
      <pubDate>Fri, 05 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://rroundtable.github.io/post/rkhs/</guid>
      <description>Hibert space  A Hilbert space($\mathbf{H}$) is a complete inner product linear space.
  Linear space  선형공간이라는 것은 임의의 두 원소 $x$,$ y$의 선형결합 또한 원소로 가지고 있어야 하는 조건이다. $$ \alpha x + \beta y \in \mathbf{H}(\alpha, \beta: scalar) $$ 2. Inner product
힐버트 공간에서는 내적으로 metric을 정의한다.
내적 $$는 $H \times H$에서 $\mathbb{R}$로의 bilinear positive definite kernel이다.
 bilinear: $x, y$ 둘 중 하나를 고정하면 다른 하나에 대한 linear 함수   positive definite    Complete  힐버트 공간 $\mathcal{H}$내의 어떠한 Cauchy sequence도 $\mathcal{H}$내의 원소로 수렴한다.</description>
    </item>
    
  </channel>
</rss>