<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on RoundTable</title>
    <link>https://rroundtable.github.io/post/</link>
    <description>Recent content in Posts on RoundTable</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>�� This post is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License��please give source if you wish to quote or reproduce.</copyright>
    <lastBuildDate>Wed, 22 Jan 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://rroundtable.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Mixup 정리글</title>
      <link>https://rroundtable.github.io/post/mixup-%EC%A0%95%EB%A6%AC%EA%B8%80f/</link>
      <pubDate>Sun, 21 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://rroundtable.github.io/post/mixup-%EC%A0%95%EB%A6%AC%EA%B8%80f/</guid>
      <description>Mixup: Beyond Empirical Risk Minimization mixup은 deep learning model의 memorization문제나 adversarial examples에 민감한 이슈를 해결하기 위해 나온 Data Augmentation기법입니다.
 memorization:  모델이 학습을 진행할 때, 정답만을 기억하고 내리는 행동. 즉, 데이터 분포를 학습하는 것이 아니라 해당 데이터가 어떤 라벨에 해당하는지 기억하게 되는 것. 결론적으로는 test distribution에 대한 generalization을 하지 못한다.
 sensitivity to adversarial examples: adversarial attack에 취약하다.  mixup은 network를 data pair간의 convex combination을 이용하여 학습시킵니다. 이는 결과적으로 모델이 training sample간에 simple linear behavior를 하지 않도록 하는 효과가 있습니다.</description>
    </item>
    
    <item>
      <title>How Does Batch Normalization Help Optimization? 정리글</title>
      <link>https://rroundtable.github.io/post/2019-07-11-how-does-batch-normalization-help-optimization/</link>
      <pubDate>Thu, 11 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://rroundtable.github.io/post/2019-07-11-how-does-batch-normalization-help-optimization/</guid>
      <description>Main Contribution Batch normalization에 대하여에서 BN이 결국 internal covariate shift현상을 해결하여, 모델의 수렴속도를 높인다고 주장하였다. 하지만, 해당 논문에서는 internal covariate shift현상을 감소하여 그러는 것이 아니며, BN이 실제로 감소시키지 않는다고 주장한다.
이 논문에서는 BN이 optimization problem을 smoother하게 만들어서 성공적이라고 주장한다. 이로 인해서 gradient는 predictive해지고 더 큰 learning rate를 사용할 수 있다.
 optimization problem이 smoother 해진다는 것은&amp;hellip;
https://ifm.mathematik.uni-wuerzburg.de/~schmidt/publications.php
 Batch normalization and internal covariate shift train, test 그래프에서는 batch normalization의 역할을 잘 보여주고 있다.</description>
    </item>
    
    <item>
      <title>What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision? 정리글</title>
      <link>https://rroundtable.github.io/post/2019-06-29-what-uncertainties-do-we-need-in-bayesian-deep/</link>
      <pubDate>Sat, 29 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://rroundtable.github.io/post/2019-06-29-what-uncertainties-do-we-need-in-bayesian-deep/</guid>
      <description>이 논문에서는 epistemic uncertainty와 aleatoric uncertainty를 하나의 모델에서 측정하는 것을 제안하고 있습니다. (이전의 연구에서는 위의 uncertainty를 따로 분리하여 측정했다고 합니다.)
  aleatoric uncertainty와 epistemic uncertainty의 차이를 보여주고 있다. 주된 차이점은 aleatoric은 물체사이의 boundary에 주로 나타나는 것을 확인할 수 있다. 맨 밑의 라인은 실패한 케이스를 보여준다. 여기서는 epistemic uncertainty가 높아진 것을 확인할 수 있다.  regression task에서 각각의 uncertainty에 대해서 알아보도록 하겠습니다.
Epistemic uncertainty  Fig.1 - Gaussian Process</description>
    </item>
    
    <item>
      <title>LogSumExp Trick</title>
      <link>https://rroundtable.github.io/post/2019-07-21-logsumexp-trick/</link>
      <pubDate>Sun, 21 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://rroundtable.github.io/post/2019-07-21-logsumexp-trick/</guid>
      <description>LogSumExp Trick 머신러닝 학습을 진행하다보면, 종종 loss가 제대로 계산되지 않는 현상이 발생한다. 이는 loss를 계산하는 과정에서 불안정한 수식을 계산하기 때문에 발생한다. 특히 cross-entropy와 같이 log함수와 연관있는 수식은 주의가 필요하다.
아래 이미지는 로그 함수 그래프이다. 이 그래프에서 알 수 있듯이 $x$의 값이 0에 가까워질수록 $\log_2(x)$의 값은 음의 무한대의 값을 가지게 되며, 컴퓨터 연산과정에서 이는 연산이 불가능하다. (overflow)  
이러한 현상을 방지하기 위해서 사용하는 것이 LogSumExp trick이다.
 LogSumExp (LSE) function is a smooth maximum – a smooth approximation to the maximum function, mainly used by machine learning algorithms.</description>
    </item>
    
    <item>
      <title>about me</title>
      <link>https://rroundtable.github.io/post/aboutme/</link>
      <pubDate>Fri, 28 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://rroundtable.github.io/post/aboutme/</guid>
      <description>Ryu Won-Tak PROJECTS Mixup implementation Data augmentation via convex combination.
Hedged instance embedding implementation Modeled uncertainty by hedging the location of each input in embedding space.
[low uncertainty in clean image] [high uncertainty in occluded image]
Error encoding network implementation Visualized that the distance of different latent variables measures the similarity of images. **Mixture density network implementation ** Visualized explainable variance and unexplainable variance. AWARDS AND HONORS  Ocean litter detector</description>
    </item>
    
    <item>
      <title>Neural Networks, Manifolds, and Topology 번역글</title>
      <link>https://rroundtable.github.io/post/neural-networks-manifolds-and-topology-/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://rroundtable.github.io/post/neural-networks-manifolds-and-topology-/</guid>
      <description>Neural Networks, Manifolds, and Topology Neural network가 실제로 어떻게 작동하는지 파악하는 것은 실제로 어려운 일입니다. 그래서 black-box 모델이라고 종종 말하곤 합니다.
이런 문제를 해결하고자 이번 글에서는 topology(math with shapes)와 neural network간의 관계를 살펴보고자 합니다. topology를 처음 접하신다면 해당 Who cares about topology?를 보시는 것을 추천드립니다.
topology를 언급하는 이유는 representation space상에서 공간의 변화가 일어나도 같은 성질을 가지고 있음을 증명하기 위해서입니다.
 Fig.1 - same topology  
도넛과 물컵은 같은 topology를 가진다.</description>
    </item>
    
    <item>
      <title>늦은 2019 회고</title>
      <link>https://rroundtable.github.io/post/2020-01-22-%EB%8A%A6%EC%9D%80-2019-%ED%9A%8C%EA%B3%A0/</link>
      <pubDate>Wed, 22 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://rroundtable.github.io/post/2020-01-22-%EB%8A%A6%EC%9D%80-2019-%ED%9A%8C%EA%B3%A0/</guid>
      <description>1. 2019년 계획했던 일 2019년은 학생에서 사회인으로 나아가는 경계였다. 2019년에는 특히 좋은 사람들을 많이 만났고, 개인적으로 많은 도움을 받은 해라고 생각한다.
(1) 딥러닝 논문 구현: 목표 달성 지금도 어렵지만, 처음에 딥러닝 논문을 구현하려고 시도했을때는 정말 큰 벽처럼 느껴졌다. 이전에는 딥러닝 구현체를 주로 가져다 쓰는 식으로 공부를 지속하였고, 돌이켜보면 전혀 성장하지 못했던 시기이다.
처음 딥러닝 논문을 구현하기로 마음먹은 것은 모두의 연구소 SafeAI랩에서 좋은 사람들을 만나서이다. 그 당시에는 구현하려는 주제가 명확했고, 누군가에게 피드백을 받을 환경이 조성되어 있어서 조금씩 성장했다.</description>
    </item>
    
    <item>
      <title>word representation</title>
      <link>https://rroundtable.github.io/post/2020-01-14-word-representation/</link>
      <pubDate>Tue, 14 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://rroundtable.github.io/post/2020-01-14-word-representation/</guid>
      <description>Word2vec: distributed representation  분산 표현(distributed representation) 방법은 기본적으로 분포 가설(distributional hypothesis)이라는 가정 하에 만들어진 표현 방법입니다. 이 가정은 &amp;lsquo;비슷한 위치에서 등장하는 단어들은 비슷한 의미를 가진다&amp;rsquo;라는 가정입니다.
reference: https://wikidocs.net/22660
 1. CBOW(Continuous Bag of Words) 주변에 있는 단어로 중간에 있는 단어를 예측하는 방법입니다. 중심단어를 예측하기 위해서 주변단어를 보는 범위를 window라고 합니다. 예를 들어서, 아래 이미지의 첫 번째 글에서 파란색 부분이 &amp;lsquo;fat&amp;rsquo;, &amp;lsquo;cat&amp;rsquo;이므로 window는 2입니다.
 보통 딥 러닝이라함은, 입력층과 출력층 사이의 은닉층의 개수가 충분히 쌓인 신경망을 학습할 때를 말하는데 Word2Vec는 입력층과 출력층 사이에 하나의 은닉층만이 존재합니다.</description>
    </item>
    
    <item>
      <title>BERT 정리글</title>
      <link>https://rroundtable.github.io/post/2019-12-30-bert-%EC%A0%95%EB%A6%AC%EA%B8%80/</link>
      <pubDate>Mon, 30 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://rroundtable.github.io/post/2019-12-30-bert-%EC%A0%95%EB%A6%AC%EA%B8%80/</guid>
      <description>BERT, Bidirectional Encoder Representations from Transformers
Abstract BERT는 unlabeled text를 기반으로 deep bidirectional representation을 pretrain하기 위해서 만들어졌다. 이는 모든 layer에서 왼쪽과 오른쪽 모두의 context정보를 바탕으로 만들어진다.
결과적으로 pretrained된 BERT 모델은 output layer을 추가하고 fine-tuning을 함으로써 question answering 그리고 language inference분야에서 state-of-the-art 성능을 낼 수 있다. 이는 task-specific한 구조의 변화없이 가능하다.
1. Introduction pretrained language representation을 활용하는데는 두 가지 전략이 있다.
 Feature-based: ELMO  task-specific한 architecture를 사용한다.
 fine-tuning: OpenAI GPT  task-specific한 parameter를 적게 사용한다.</description>
    </item>
    
    <item>
      <title>Big-O notation 정리하기</title>
      <link>https://rroundtable.github.io/post/2019-12-28-big-o-notation-%EC%A0%95%EB%A6%AC%ED%95%98%EA%B8%B0/</link>
      <pubDate>Sat, 28 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://rroundtable.github.io/post/2019-12-28-big-o-notation-%EC%A0%95%EB%A6%AC%ED%95%98%EA%B8%B0/</guid>
      <description>In computer science, big O notation is used to classify algorithms according to how their running time or space requirements grow as the input size grows.[3] In analytic number theory, big O notation is often used to express a bound on the difference between an arithmetical function and a better understood approximation; a famous example of such a difference is the remainder term in the prime number theorem.</description>
    </item>
    
    <item>
      <title>Sorting Algorithm</title>
      <link>https://rroundtable.github.io/post/2019-12-28-sorting-algorithm/</link>
      <pubDate>Sat, 28 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://rroundtable.github.io/post/2019-12-28-sorting-algorithm/</guid>
      <description>Content  Insert sort Merge sort Quick sort Heap sort  Insert Sort insert sort는 일반적으로 적은 수의 요소들을 정렬할 때 유리하다.
insert sort는 배열이 주어졌을 때, 순차적으로 순회하면서 각 요소의 올바른 위치로 정렬한다. 만약, i 번째까지 순회하였다면, 0 ~ i번째의 배열은 0 ~ i번째 배열의 성분 기준으로 모두 알맞은 위치에 정렬된 상태이다. i + 1번째 요소는 알맞게 정렬된 0 ~ i번째 배열기준으로 알맞은 위치로 삽입된다.
이를 의사코드로 나타내면 다음과 같다.</description>
    </item>
    
    <item>
      <title>attention is all you need 정리글</title>
      <link>https://rroundtable.github.io/post/2019-11-18-attention-is-all-you-need-%EC%A0%95%EB%A6%AC%EA%B8%80/</link>
      <pubDate>Mon, 18 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://rroundtable.github.io/post/2019-11-18-attention-is-all-you-need-%EC%A0%95%EB%A6%AC%EA%B8%80/</guid>
      <description>Transformer 구조 위의 이미지는 Transformer의 구조이다. encoder와 decoder 구조로 이루어져 있다. 마지막 encoder layer의 output이 각 decoder stack에 input으로 들어가게 된다. (residual connection) 각 encoder layer와 decoder layer는 모두 동일한 구조를 가지나 서로 parameter를 공유하지 않는다. 아래의 그림처럼 논문에서는 각 6개의 layer를 가지고 있다.
아래의 이미지는 encoder와 decoder의 세부 구조이다.
각 세부 layer사이에는 Normalization 및 bias를 더하는 과정이 추가된다.
 Matrix Calculation of Self-Attention 이제 복수의 embeddinb vector를 matrix 연산으로 대체하는 과정을 살펴보자.</description>
    </item>
    
    <item>
      <title>Unsupervised Monocular Depth Estimation with Left-Right Consistency 정리글</title>
      <link>https://rroundtable.github.io/post/2019-11-04-unsupervised-monocular-depth-estimation-with-left-right-consistency-%EC%A0%95%EB%A6%AC%EA%B8%80/</link>
      <pubDate>Mon, 04 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://rroundtable.github.io/post/2019-11-04-unsupervised-monocular-depth-estimation-with-left-right-consistency-%EC%A0%95%EB%A6%AC%EA%B8%80/</guid>
      <description>Abstract 많은 방법론들이 depth estimation부분에서 성과를 보여주었다. 하지만, 대부분 지도학습이라는 한계를 가지고 있으며, 이는 결국 많은 수의 ground-truth data가 필요하다는 것을 의미한다. 하지만, depth를 기록하는 것은 매우 어려운 문제이다. 따라서 이 연구에서는 얻기 쉬운 binocular stereo footage를 이용하여 문제를 해결한다.
epipolar geometry constraints를 활용해서 reconstruction loss로 학습을 시킬 수 있다. 하지만 이 결과물은 depth image의 질이 낮아진다. 이러한 문제를 해결하기 위해서, consistency를 유지할 수 있게하는 loss를 제안한다.
 Epipolar geometry</description>
    </item>
    
    <item>
      <title>Feature Visulatization 번역글</title>
      <link>https://rroundtable.github.io/post/2019-10-29-feature-visulatization-%EB%B2%88%EC%97%AD%EA%B8%80/</link>
      <pubDate>Tue, 29 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://rroundtable.github.io/post/2019-10-29-feature-visulatization-%EB%B2%88%EC%97%AD%EA%B8%80/</guid>
      <description>Introduction neural network의 해석가능성에 대한 필요성이 늘어나고 있다. Deep learning의 해석가능성은 크게 두 가지 문제로 나뉜다.
 feature visualization  network 혹은 network의 부분이 무엇을 보고자 하는가
 attribution  network가 다음과 같이 동작하는 이유가 무엇인가
class activation map이 하나의 예시가 될 수 있다.
Feature Visualization by Optimization 일반적으로 neural network는 input에 대해서 differentiable하다. 만약 당신이 어떤 종류의 input이 특정한 행동양상을 가지는지 알고 싶다면(내부적인 뉴런의 동작 혹은 마지막 결과물의 양상이 예시가 될 수 있다.</description>
    </item>
    
    <item>
      <title>tensorrt 정리글</title>
      <link>https://rroundtable.github.io/post/2019-09-08-tensorrt-%EC%A0%95%EB%A6%AC%EA%B8%80/</link>
      <pubDate>Sun, 08 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://rroundtable.github.io/post/2019-09-08-tensorrt-%EC%A0%95%EB%A6%AC%EA%B8%80/</guid>
      <description>[1.3. How Does TensorRT Work?] inference 과정을 최적화 시키기 위해서, TensorRT는 network definition을 가져와서 해당 환경(GPU)에서 최적화를 하며, inference engine을 생성한다. 이 과정은 상당한 시간이 소요되며, embedded 된 platform(하드웨어)에서는 더 오래걸린다. 이런 이유 때문에, 보통 engine을 만들면 그것을 serialize화 하여, 저장하고 후에 읽어와서 사용하는 방법을 선호한다.
Note: 위의 generated된 file은 다른 tensorRT 버전 혹은 다른 플렛폼에서 사용할 수 없다. 특정 gpu에서만 사용해야한다.
 serialize
직렬화(直列化) 또는 시리얼라이제이션(serialization)은 컴퓨터 과학의 데이터 스토리지 문맥에서 데이터 구조나 오브젝트 상태를 동일하거나 다른 컴퓨터 환경에 저장(이를테면 파일이나 메모리 버퍼에서, 또는 네트워크 연결 링크 간 전송)하고 나중에 재구성할 수 있는 포맷으로 변환하는 과정이다.</description>
    </item>
    
    <item>
      <title>Activation atlas 정리글</title>
      <link>https://rroundtable.github.io/post/activation-atlas/</link>
      <pubDate>Mon, 26 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://rroundtable.github.io/post/activation-atlas/</guid>
      <description>Introduction What have these networks learned that allows them to classify images so well?
네트워크가 classification을 잘하는 이유를 찾기 위해서 다음과 같은 시도를 하였다.
기본적으로 네트워크를 시각적으로 분석할려고 노력했다.
 individual neurons  뉴런들을 독립적으로 시각화
 Interaction between Neurons  뉴런은 독립적으로 움직이는 것이 아니기 때문에 simple feature combination을 시각화함. 이런 시도는 문제점을 가지고 있었다.
예를 들어, 수 많은 combination중 어떤 combination을 살펴봐야하는지 어떻게 알 수 있는가?
 spatial activation  위의 질문에 대한 답은 activation을 시각화하는 것에 있다.</description>
    </item>
    
    <item>
      <title>MixConv: Mixed Depthwise Convolutional Kernels</title>
      <link>https://rroundtable.github.io/post/mixconv/</link>
      <pubDate>Fri, 23 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://rroundtable.github.io/post/mixconv/</guid>
      <description>Abstract  다양한 kernel size의 효과  다양한 kernel size의 조합은 모델의 accuracy 및 efficient를 향상 시킬 수 있다.  위를 바탕으로 depth-wise convolution(MixConv)제안 MixConv의 효과를 증명하기 위해서 AutoML을 결합  Introduction Figure 1에서 볼 수 있듯이, kernel size가 클수록 모델의 성능이 올라간다고 할 수 없다. 올라가는 추세를 보이다가 k9*9를 넘어서면 accuracy가 떨어지는 것을 확인할 수 있다.
ConvNets 연구에서는 하나의 kernel size의 한계를 말한다. 결국 high-resolustion pattern을 위해서는 큰 kernel size가 필요하고 local-resolution pattern을 위해서는 작은 kernel-size가 필요하다.</description>
    </item>
    
    <item>
      <title>Python 변수할당의 개념</title>
      <link>https://rroundtable.github.io/post/2019-08-03-python-%EB%B3%80%EC%88%98%ED%95%A0%EB%8B%B9%EC%9D%98-%EA%B0%9C%EB%85%90/</link>
      <pubDate>Sat, 03 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://rroundtable.github.io/post/2019-08-03-python-%EB%B3%80%EC%88%98%ED%95%A0%EB%8B%B9%EC%9D%98-%EA%B0%9C%EB%85%90/</guid>
      <description>Python은 C언어와 다르게 Pointer가 존재하지 않는다.  Pointer란
프로그래밍 언어에서 다른 변수, 혹은 그 변수의 메모리 공간주소를 가리키는 변수를 말한다. 포인터가 가리키는 값을 가져오는 것을 역참조라고 한다.
https://ko.wikipedia.org/wiki/%ED%8F%AC%EC%9D%B8%ED%84%B0_(%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D)
 python에서는 메모리 주소를 변수를 구별하는 용도로 사용한다.
참고로 변수의 메모리 주소는 id()를 이용하여 구할 수 있다.
a = &amp;#34;String&amp;#34; id(a) python에서는 변수에 값을 할당하면 object를 생성해서 값을 저장한 후 변수는 해당 object의 메모리 주소를 의미하는 id값을 가지게 된다. 여기서 변수는 object의 label정도로 생각하면 된다.</description>
    </item>
    
    <item>
      <title>Dropout as Bayesian Approximation 정리글</title>
      <link>https://rroundtable.github.io/post/2019-08-01-dropout-as-bayesian-approximation-%EC%A0%95%EB%A6%AC%EA%B8%80/</link>
      <pubDate>Thu, 01 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://rroundtable.github.io/post/2019-08-01-dropout-as-bayesian-approximation-%EC%A0%95%EB%A6%AC%EA%B8%80/</guid>
      <description>Problem 일반적으로 Bayesain model은 model의 uncertainty를 측정할 수 있다는 장점이 있지만, computation cost가 너무 커서 사용하기 힘들다는 문제를 가지고 있다. 이런 문제점을 해결하기 위해서 이 논문에서는 Dropout을 사용한 딥러닝 모델이 결국은 gaussian porcess에서의 bayesain inference를 근사한 것이라는 증명을 할 것이다.
Related Research  Bayesian learning for neural networks  infinite-wide neural network에 distribution을 가정하면, 결국 gaussian process를 approxiation하는 것이다. 하지만 finite-wide neural network에서는 증명되지 않았다.
추가적으로 finite-wide neural network상에서 연구되었다.</description>
    </item>
    
    <item>
      <title>mesh tensorflow 정리글</title>
      <link>https://rroundtable.github.io/post/2019-07-30-mesh-tensorflow-%EC%A0%95%EB%A6%AC%EA%B8%80/</link>
      <pubDate>Tue, 30 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://rroundtable.github.io/post/2019-07-30-mesh-tensorflow-%EC%A0%95%EB%A6%AC%EA%B8%80/</guid>
      <description>요약 batch-spliting이란, data-paralleism 방법론으로 분산화된 딥러닝 네트워크에서 많이 사용하며, Single-Program-Multiple-Data programing의 일종이다. 즉, 데이터가 클 때 분산시켜서 대처하는 방법론이라고 할 수 있다.
하지만, 모델이 한번에 RAM에 올리기 클 경우에는 어떻게 해야할까? 혹은 모델의 크기 때문에 작은 batch size를 사용할 때 발생하는 high latency와 비효율성이 발생한다면 어떻게 해야할까? 이를 해결하기 위해서는 Model-parallenism을 사용해야한다.
하지만, 효과적인 model-parallelism은 일반적으로 복잡한 편이다. 이런 문제를 간단하게 해결하기 위해서 Mesh-tensorflow를 제안한다. data-parallelism은 tensor와 operations를 batch dimension으로 나누는 것으로 치환한다.</description>
    </item>
    
    <item>
      <title>Faster-RCNN 정리글</title>
      <link>https://rroundtable.github.io/post/2019-07-26-faster-rcnn-%EC%A0%95%EB%A6%AC%EA%B8%80/</link>
      <pubDate>Thu, 25 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://rroundtable.github.io/post/2019-07-26-faster-rcnn-%EC%A0%95%EB%A6%AC%EA%B8%80/</guid>
      <description>Problem: bottleneck 기존의 state-of-the-art object detection network는 region proposal algorithm을 사용하였다. (ex-Fast R-CNN) region proposal algorithm은 이들 network상에서 bottleneck의 역할을 하고 있었다. 즉, region proposal algorithm 때문에 학습 시간 및 알고리즘 수행시간이 지체되고 있는 것을 확인했다.
이러한 문제를 해결하기 위해서 Region Proposal Network를 제안하는데 이는 full-image convolutional feature를 region proposal하는데도 사용하여 cost-free하게 적용될 수 있다.
 Fast-RCNN: region proposal algorithm
selective search 한 이미지 당 cpu기준 약 2초의 시간이 걸린다.</description>
    </item>
    
    <item>
      <title>Mixture density Network: Uncertainty estimation</title>
      <link>https://rroundtable.github.io/post/2019-07-26-mixture-density-network-uncertainty-estimation/</link>
      <pubDate>Thu, 25 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://rroundtable.github.io/post/2019-07-26-mixture-density-network-uncertainty-estimation/</guid>
      <description>Problem 기존의 방법론들은 uncetrainty를 estimate하는데 sampling을 해야하는 한계가 있다. 이 논문에서는 gaussian mixure model을 활용한 sampling-free uncertainty estimation방법론을 제안한다.
Mixture Density Network   위의 이미지처럼 MDN은 output이 fixed value가 아닌 distribution의 형태를 가진다. 이런 특성 때문에 multi modal한 상황에서도 학습이 잘 진행된다.   MDN을 수식으로 나타내면 아래와 같다. $$ p(y|\theta) = \sum_{j=1}^K\pi_j\mathcal{N}(y|u_j, \Sigma_j) $$ where
 $\theta=\{\pi_j, u_j ,\Sigma_j \}_{j=1}^K$
 $\pi_j$: 가중치, 0 ~ 1사이의 값,$\sum_{j=1}^K\pi_j = 1$</description>
    </item>
    
    <item>
      <title>Batch Normalization에 대하여</title>
      <link>https://rroundtable.github.io/post/2019-07-08-batch-normalization%EC%97%90-%EB%8C%80%ED%95%98%EC%97%AC/</link>
      <pubDate>Mon, 08 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://rroundtable.github.io/post/2019-07-08-batch-normalization%EC%97%90-%EB%8C%80%ED%95%98%EC%97%AC/</guid>
      <description>Problem Define 학습하는 과정에서 이전 layer의 parameter가 변하면서, 각 layer의 input들의 distribution이 training과정마다 변하게 된다. 이런 문제는 학습이 불안정하게 하며, 낮은 learning rate를 사용해야 학습이 진행된다. 결론적으로는 saturating non-linearity의 모델을 학습하기 어려워진다. 이런 현상을 internal covariate shift 라고 부른다.
 saturating non-linearity: 어떤 입력이 무한대로 갈 때 함수값이 어떤 범위내에서만 움직이는 것
ex) sigmoid
not-saturating non-linearity: 어떤 입력이 무한대로 갈 때 함수값도 무한대로 가는 것을 의미
ex) Relu
 sigmoid activation에 대해서 생각해보면, 위의 문제가 왜 심각한지 알 수 있다.</description>
    </item>
    
    <item>
      <title>MLE 와 MAP에 대하여</title>
      <link>https://rroundtable.github.io/post/mle-maximum-likelihood-estimation/</link>
      <pubDate>Sun, 07 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://rroundtable.github.io/post/mle-maximum-likelihood-estimation/</guid>
      <description>MLE: Maximum Likelihood Estimation *liklihood*란, 이미 주어진 표본적 증거로 비추어보았을 때, 모집단에 관해 어떠한 통계적 추정이 그럴듯한 정도를 의미합니다. 수식으로 나타내면, $P(D|W)$ 처럼 나타낼 수 있다. (D = observation, W = parameters) .
동전던지기 예시를 생각해보면, 쉽게 이해할 수 있다. 일반적으로 동전 던지기를 해서 앞면이 나올 확률은 0.5라고 생각합니다. 하지만, 이는 우리가 가정한 값이지 실제의 값은 아닙니다. 이런 이유로 몇 번의 수행결과로 동전의 앞면이 나올 확률 $P(H)$를 정의하고자 합니다.</description>
    </item>
    
    <item>
      <title>RKHS에 대하여</title>
      <link>https://rroundtable.github.io/post/rkhs/</link>
      <pubDate>Fri, 05 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://rroundtable.github.io/post/rkhs/</guid>
      <description>Hibert space  A Hilbert space($\mathbf{H}$) is a complete inner product linear space.
  Linear space  선형공간이라는 것은 임의의 두 원소 $x$,$ y$의 선형결합 또한 원소로 가지고 있어야 하는 조건이다. $$ \alpha x + \beta y \in \mathbf{H}(\alpha, \beta: scalar) $$ 2. Inner product
힐버트 공간에서는 내적으로 metric을 정의한다.
내적 $$는 $H \times H$에서 $\mathbb{R}$로의 bilinear positive definite kernel이다.
 bilinear: $x, y$ 둘 중 하나를 고정하면 다른 하나에 대한 linear 함수   positive definite    Complete  힐버트 공간 $\mathcal{H}$내의 어떠한 Cauchy sequence도 $\mathcal{H}$내의 원소로 수렴한다.</description>
    </item>
    
    <item>
      <title>Image segmentation에서 사용되는 loss</title>
      <link>https://rroundtable.github.io/post/image-segmentation-loss/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://rroundtable.github.io/post/image-segmentation-loss/</guid>
      <description>Basic: Cross Entropy 일반적으로 classification문제에서 사용되는 term이다. $$ CE(p, \hat{p}) = -(p \log(\hat{p}) + (1- p)\log(1-\hat{p})) $$
Weighted cross entropy $$ WCE(p, \hat{p}) = -(\beta p \log(\hat{p}) + (1- p)\log(1-\hat{p})) $$
위의 cross entropy의 변형으로 모든 positive samples는 가중치를 부여 받는다. 이는 class imbalace문제를 다루기 위해서 사용한다. 만약 positive sample이 is 90%고 negative sample이 10%라면 cross entropy는 잘 작동하기 힘들다. 위와 같은 상황에서는 positive sample에 $\beta$ 를 1보다 작은 값을 부여하여 해결하고 만약 negative sample이 더 많다면 반대로 $\beta$ 에 1보다 큰 값을 부여하여 해결한다.</description>
    </item>
    
  </channel>
</rss>